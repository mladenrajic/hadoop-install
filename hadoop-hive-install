#! /bin/sh

# UPDATE & UPGRADE Ubuntu Server
sudo apt update -y
sudo apt upgrade -y

# CLEAR screen
clear

# MOVE TO HOME DIRECTORY OF CURRENT USER (place of instalation)
cd ~

# SETUP SSH for pseudodistributed hadoop instalation
echo "\n\n\n" | ssh-keygen -y
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

# REMOVE STRICT HOST CHECK WHEN hadoop does ssh
echo "Host *" > ~/.ssh/config
echo "    StrictHostKeyChecking no" >> ~/.ssh/config
#sudo chmod 400 ~/.ssh/config

# INSTALL OPEN-JDK-8
sudo apt install openjdk-8-jdk openjdk-8-jre-headless -y
java -version

# DOWNLOAD HADOOP 3.2.1
[ -d hadoop ] && rm -r hadoop
[ -f hadoop-3*.gz ] && rm hadoop-3*.gz
wget https://downloads.apache.org/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz
tar xvzf hadoop-3.2.1.tar.gz
mv hadoop-3.2.1 hadoop
rm hadoop-3.2.1.tar.gz
mkdir ~/hadoop/namenode
mkdir ~/hadoop/datanode


# DOWNLOAD HIVE 3.1.2
[ -d hive ] && rm -r hive
[ -f apache-hive-3.1.2-bin.tar.gz ] && rm apache-hive-3.1.2-bin.tar.gz
wget https://downloads.apache.org/hive/hive-3.1.2/apache-hive-3.1.2-bin.tar.gz
tar xvzf apache-hive-3.1.2-bin.tar.gz
mv apache-hive-3.1.2-bin hive
rm apache-hive-3.1.2-bin.tar.gz


# SETUP HADOOP_HOME variable to profile and update the path
echo "" > .hadoop_aliases
echo "# HADOOP SETUP" >> .hadoop_aliases
echo "export HADOOP_HOME=\"$PWD/hadoop\"" >> .hadoop_aliases
echo "" >> .hadoop_aliases

echo "# HIVE SETUP" >> .hadoop_aliases
echo "export HIVE_HOME=\"$PWD/hive\"" >> .hadoop_aliases
echo 'export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin' >> .hadoop_aliases
echo "" >> .hadoop_aliases

# PROPAGATE CHANGES TO .bashrc
sed -i '/# HADOOP AND HIVE SETUP/d' .bashrc
sed -i '/[ -f "~\/\.hadoop_aliases" ] && \. ~\/\.hadoop_aliases/d' .bashrc

echo "# HADOOP AND HIVE SETUP" >> .bashrc
echo "[ -f "~/.hadoop_aliases" ] && . ~/.hadoop_aliases" >> .bashrc
echo "" >> .bashrc



# ADD JAVA_HOME path to hadoop-env.sh script
echo "" >> ~/hadoop/etc/hadoop/hadoop-env.sh
echo "# JAVA_HOME setup" >> ~/hadoop/etc/hadoop/hadoop-env.sh
echo "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre" >> ~/hadoop/etc/hadoop/hadoop-env.sh


# REMOVE OLD JARS FROM HIVE LOGGING SYTEM
rm hive/lib/log4j-slf4j-impl-2.10.0.jar
rm hive/lib/guava-19.0.jar
cp hadoop/share/hadoop/common/lib/guava-27.0-jre.jar hive/lib



# SETUP CONFIG FILES FOR HADOOP
echo '<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>' > hadoop/etc/hadoop/core-site.xml

echo "<?xml version=\"1.0\" encoding=\"UTF-8\"?>
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
        <description>
            Default block replication.
            The actual number of replications can be specified when the file is created.
            The default is used if replication is not specified in create time.
        </description>
    </property>
    <property>
        <name>dfs.blocksize</name>
        <value>134217728</value>
        <description>
            Hadoop block size - 128MB.
        </description>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///home/$(whoami)/hadoop/namenode</value>
        <description>
            Hadoop directory for namenode metadata store.
        </description>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///home/$(whoami)/hadoop/datanode</value>
        <description>
            Hadoop directory for data node block store.
        </description>
    </property>
</configuration>" > hadoop/etc/hadoop/hdfs-site.xml




echo "<?xml version=\"1.0\"?>
<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property> 
        <name>mapreduce.application.classpath</name>
        <value>/home/$(whoami)/hadoop/share/hadoop/mapreduce/*,/home/$(whoami)/hadoop/share/hadoop/mapreduce/lib/*,/home/$(whoami)/hadoop/share/hadoop/common/*,/home/$(whoami)/hadoop/share/hadoop/common/lib/*,/home/$(whoami)/hadoop/share/hadoop/yarn/*,/home/$(whoami)/hadoop/share/hadoop/yarn/lib/*,/home/$(whoami)/hadoop/share/hadoop/hdfs/*,/home/$(whoami)/hadoop/share/hadoop/hdfs/lib/*</value>
    </property>
</configuration>" > hadoop/etc/hadoop/mapred-site.xml



echo '<?xml version="1.0"?>
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configuration>' > hadoop/etc/hadoop/yarn-site.xml




# HIVE CONF SETUP
echo "<?xml version=\"1.0\"?>
<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>
<configuration>
    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse</value>
        <description>HDFS directory where Hive keeps table contents.</description>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:derby:;databaseName=/home/$(whoami)/hive/metastore_db;create=true</value>
        <description>The JDBC connection URL.</description>
    </property>
</configuration>" > hive/conf/hive-site.xml

